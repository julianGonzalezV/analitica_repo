Curso de google cloud platform: Coursera

:::Hadoop administrado en la nube:::::
El administrador sería Dataproc que gestiona todo lo relacionado con Hadoop, pig(ETL scripts), hive(offer a sql context) and spark 

En lugar de almacenar los datos en archivos HDFS hágalo en google cloud storage :
-HDFs:Toma los datos y los divide en el cluster Dataproc, lo que requieren que el luster esté arriba porque si se elimina entoncs la informacion no estáa disponible o incluso se pierd
Además al estar la infomacion distribuida en el cluster y el cluster tener que estar arribar(porque no queremos perder la informaci'on), el costo (facturación mensual) va a ser mayor
Con lo anterior estamos pagando (24/7)solo por storage así el cluster no esté ejecutandose (corriendo jobs)


-Google Cloud Storage: El ciclo de vida del cluster(data proc cluster) y el ciclo de vida del storage están separados (note ariba que no es así ya que si el uno está abajo la información no está disponible), ASI QUE podemos encender el luster procesar la información y hacer delete del cluster(para ahorrar costos) y la info sigu en GCS
Almacene la info idealmente en una sola region de bucket y cree el dataproc cluster en esa misma region


Instancia mysql::::::::::::::::::::::::::::::::::::

Instancia mysql
user root
pwd root321
35.196.82.212


Cloud Shell (cliente)
(Paso 7)
Cloud SQL (servidor MySQL)
(Paso 10)
::::::::::::::::::::::::::::::::::::::::::::::::::


<<<<<<< HEAD
OJO EMPEZAR A ELIMINAR SERVICIOS PORQUE YA NOS ESTÁ BAJANDO EL PRESUPUESTO GRATIS 



iniciar una vm
datalab connect mydatalabvm


::Para cambiar de zona a una VM::::::::::

gcloud compute instances move mydatalabvm --zone us-central1-b --destination-zone us-central1-f

::::::::::::::::::::::::::::::::::::::::::::::::::TENSOR FLOW ;::::::::::::::::::::::::::::::::::::..
ml library that google open sourced in 2015 , es bśicamente una librería de procesamiento numérico


Que es una red neuronal? Neural network?:



:::::.Feature enginerign 
Agregar nuevas caracteristicas O INSIGHTS o conocimientos que hacen que se afine la predición a traves de las iteraciones


OTRA opción para hacer prediciones es crear una red neuronal 










=======
:::2018-10-30: Cloud Datastore::::::::::::::::::::::::::::::::::::::::::::::
Nos ayuda con el tratamiendo de grandes volumens de información:
Inicialmente teníamos bases de datos relacionales y mapeado por ejemplo en un backend como objetos pero si la tabla se llama player y tiene un tipo 
entonces aparece la famosa IMPEDANCIA, que no es más que por el modelo objectual tndriamos la clase Player pero 2 clases eje Futbol o Tennis y heredando de 
player pero en BD es una sola columna tipo!!

Datastore permite almacenar el objeto completo ! escala hasta Terabytes de datos
Datastore interaction: DataStore es como un Hashmap

BigTable: Niveles de PetaBytes
Bigtable seems to be designed for HBase compatibility, whereas Datastore is more geared towards Python/Java/Go web app developers (originally App Engine)
Bigtable is 'a bit more IaaS' than Datastore in that it's not 'just there' but requires a cluster to be configured.
Bigtable supports only one index - the 'row key' (the entity key in Datastore)
This means queries are on the Key, unlike Datastore's indexed properties
Bigtable supports atomicity only on a single row - there are no transactions
Mutations and deletions appear not to be atomic in Bigtable, whereas Datastore provides eventual and strong consistency, depending on the read/query method
The billing model is very different:
Datastore charges for read/write operations, storage and bandwidth
Bigtable charges for 'nodes', storage and bandwidth

::::::DATA LAB::::::::::::::::
aDEMÁS DE Lo anterior esta es otra herramienta tecnológica que podemos usar

DataLab es un notebook open suource solo pagamos por los recursos de GCP

Opciones para trabajar con DataLab:
1) Locally : Own cpu, instalando la imagen de docker que contiene el datalab. Buena idea pero si vas a probar o eres el único que vas a usar ese Notebook 
2) El notebook instalado en CE (compue engine) y conectandonos con shell al cloud shell y todo lo que corramos se realiza en el compue Engine Instance
3) 
>>>>>>> c9095a4964689a6f303c2153c26f2859c11f4f0c
